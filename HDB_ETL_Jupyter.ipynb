{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-7873080754854529896",
   "metadata": {},
   "source": [
    "# HDB Resale Flat Prices ‚Äî ETL Pipeline\n",
    "\n",
    "This notebook runs the full end-to-end ETL pipeline for HDB resale flat prices (Mar 2012 ‚Äì Dec 2016).\n",
    "\n",
    "### Pipeline Stages\n",
    "| Stage | Description |\n",
    "|-------|-------------|\n",
    "| **0. Download** | Fetch raw CSVs from data.gov.sg via API |\n",
    "| **1. Load** | Read and align both CSV snapshots |\n",
    "| **2.5. Profile (Raw)** | Statistical summary of raw master dataset (pre-cleaning baseline) |\n",
    "| **3. Clean** | Type casting, null drops, lease recomputation |\n",
    "| **4. Deduplicate** | Remove duplicate records, save audit file |\n",
    "| **5. Validate** | Apply business rules, flag violations |\n",
    "| **6. Anomaly Detection** | 3-sigma price outlier detection per town/flat type |\n",
    "| **7. Profile (Cleaned)** | Statistical summary of final cleaned dataset (post-cleaning) |\n",
    "| **8. Transform** | Create synthetic Resale Identifier |\n",
    "| **9. Hash** | SHA-256 hash the Resale Identifier |\n",
    "\n",
    "> **Prerequisites:** Ensure `download_hdb_data.py` is in the same folder as this notebook.\n",
    "> Install dependencies: `pip install pandas requests`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-3283083262286261220",
   "metadata": {},
   "source": [
    "## Stage 0 ‚Äî Download Raw Data\n",
    "\n",
    "> **How this works:** `%run` executes `download_hdb_data.py` directly inside the notebook kernel,\n",
    "> so all `print()` output appears here in real time.\n",
    "\n",
    "> **Requirement:** `download_hdb_data.py` must be in the **same folder** as this notebook.\n",
    "\n",
    "The script will:\n",
    "1. Connect to the data.gov.sg API\n",
    "2. Auto-discover matching datasets by keyword\n",
    "3. Download both CSV files into the `hdb_data/` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-971733223855965915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ‚îÄ‚îÄ Preflight check ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "DOWNLOADER = 'download_hdb_data.py'\n",
    "\n",
    "if not os.path.isfile(DOWNLOADER):\n",
    "    raise FileNotFoundError(\n",
    "        f\"'{DOWNLOADER}' not found in '{os.getcwd()}'.\\n\"\n",
    "        f\"Please copy download_hdb_data.py into the same folder as this notebook.\"\n",
    "    )\n",
    "\n",
    "print(f'‚úì {DOWNLOADER} found ‚Äî starting download...')\n",
    "print('=' * 60)\n",
    "\n",
    "# ‚îÄ‚îÄ Run downloader in-kernel so all output is visible ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "%run download_hdb_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-6428715373487472338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Verify downloaded files exist before proceeding ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "EXPECTED_FILES = [\n",
    "    os.path.join('hdb_data', 'Resale_Flat_Prices_Based_on_Registration_Date_From_Mar_2012_to_Dec_2014.csv'),\n",
    "    os.path.join('hdb_data', 'Resale_Flat_Prices_Based_on_Registration_Date_From_Jan_2015_to_Dec_2016.csv'),\n",
    "]\n",
    "\n",
    "print('Checking downloaded files...')\n",
    "all_found = True\n",
    "for f in EXPECTED_FILES:\n",
    "    if os.path.isfile(f):\n",
    "        size_kb = os.path.getsize(f) / 1024\n",
    "        print(f'  ‚úì {f}  ({size_kb:.1f} KB)')\n",
    "    else:\n",
    "        print(f'  ‚ùå MISSING: {f}')\n",
    "        all_found = False\n",
    "\n",
    "if not all_found:\n",
    "    raise RuntimeError('Some files are missing. Re-run the download cell above before continuing.')\n",
    "else:\n",
    "    print('\\n‚úÖ All files present ‚Äî safe to proceed to Stage 1.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-1661948964046021057",
   "metadata": {},
   "source": [
    "## Stage 1 ‚Äî Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-672414232777651006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print('‚úì Libraries imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-7808428785612355360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Output directories ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "RAW_DIR          = 'hdb_data'\n",
    "OUTPUT_DIR       = 'output'\n",
    "RAW_OUT_DIR      = os.path.join(OUTPUT_DIR, 'raw')\n",
    "CLEANED_OUT_DIR  = os.path.join(OUTPUT_DIR, 'cleaned')\n",
    "TRANSFORM_OUT_DIR= os.path.join(OUTPUT_DIR, 'transformed')\n",
    "HASHED_OUT_DIR   = os.path.join(OUTPUT_DIR, 'hashed')\n",
    "FAILED_OUT_DIR   = os.path.join(OUTPUT_DIR, 'failed')\n",
    "AUDIT_OUT_DIR    = os.path.join(OUTPUT_DIR, 'audit')\n",
    "PROFILE_OUT_DIR  = os.path.join(OUTPUT_DIR, 'profiling')\n",
    "\n",
    "for d in [RAW_OUT_DIR, CLEANED_OUT_DIR, TRANSFORM_OUT_DIR,\n",
    "          HASHED_OUT_DIR, FAILED_OUT_DIR, AUDIT_OUT_DIR, PROFILE_OUT_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print('‚úì Output directories created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-6755739720814924498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Input CSV files ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "CSV_FILES = [\n",
    "    'Resale_Flat_Prices_Based_on_Registration_Date_From_Mar_2012_to_Dec_2014.csv',\n",
    "    'Resale_Flat_Prices_Based_on_Registration_Date_From_Jan_2015_to_Dec_2016.csv'\n",
    "]\n",
    "CSV_PATHS = [os.path.join(RAW_DIR, f) for f in CSV_FILES]\n",
    "\n",
    "EXPECTED_START = pd.Period('2012-03', freq='M')\n",
    "EXPECTED_END   = pd.Period('2016-12', freq='M')\n",
    "\n",
    "print('‚úì CSV paths configured')\n",
    "for p in CSV_PATHS:\n",
    "    exists = '‚úì Found' if os.path.isfile(p) else '‚ùå Missing'\n",
    "    print(f'  {exists}: {p}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-4053748763826909990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Validation reference sets ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "VALID_TOWNS = {\n",
    "    'ANG MO KIO','BEDOK','BISHAN','BUKIT BATOK','BUKIT MERAH',\n",
    "    'BUKIT PANJANG','BUKIT TIMAH','CENTRAL AREA','CHOA CHU KANG','CLEMENTI',\n",
    "    'GEYLANG','HOUGANG','JURONG EAST','JURONG WEST','KALLANG/WHAMPOA',\n",
    "    'MARINE PARADE','PASIR RIS','PUNGGOL','QUEENSTOWN','SEMBAWANG',\n",
    "    'SENGKANG','SERANGOON','TAMPINES','TOA PAYOH','WOODLANDS','YISHUN'\n",
    "}\n",
    "\n",
    "VALID_FLAT_TYPES = {'1 ROOM','2 ROOM','3 ROOM','4 ROOM','5 ROOM','EXECUTIVE','MULTI-GENERATION'}\n",
    "\n",
    "VALID_FLAT_MODELS = {\n",
    "    '2-room','Adjoined flat','Apartment','DBSS','Improved','Improved-Maisonette',\n",
    "    'Maisonette','Model A','Model A2','Model A-Maisonette','Multi Generation',\n",
    "    'New Generation','Premium Apartment','Premium Apartment Loft','Premium Maisonette',\n",
    "    'Simplified','Standard','Terrace','Type S1','Type S2'\n",
    "}\n",
    "\n",
    "VALID_STOREY_FORMAT = r'^\\d{2} TO \\d{2}$'\n",
    "\n",
    "print(f'‚úì Validation sets loaded: {len(VALID_TOWNS)} towns, {len(VALID_FLAT_TYPES)} flat types, {len(VALID_FLAT_MODELS)} flat models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-3574869889449696156",
   "metadata": {},
   "source": [
    "## Stage 2 ‚Äî Load & Align Raw Snapshots\n",
    "\n",
    "Reads both CSV files and aligns their columns (via `reindex`) before concatenating into one master DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-8903133000836065713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timed_step(name, func, *args, **kwargs):\n",
    "    start = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    elapsed = time.time() - start\n",
    "    print(f'‚è± {name} executed in {elapsed:.2f}s')\n",
    "    return result\n",
    "\n",
    "def load_and_align_snapshots():\n",
    "    print('üì¶ Loading and aligning CSV snapshots...')\n",
    "    dfs = []\n",
    "    for path in CSV_PATHS:\n",
    "        print(f'   - {path}')\n",
    "        df = pd.read_csv(path)\n",
    "        print(f'     ‚Üí {len(df):,} rows, {len(df.columns)} columns')\n",
    "        dfs.append(df)\n",
    "    all_columns = sorted(set().union(*(df.columns for df in dfs)))\n",
    "    aligned_dfs = [df.reindex(columns=all_columns) for df in dfs]\n",
    "    master_df = pd.concat(aligned_dfs, ignore_index=True)\n",
    "    return master_df\n",
    "\n",
    "df = timed_step('Load & Align CSVs', load_and_align_snapshots)\n",
    "\n",
    "raw_file = os.path.join(RAW_OUT_DIR, 'hdb_resale_raw.csv')\n",
    "df.to_csv(raw_file, index=False)\n",
    "print(f'üíæ Raw dataset saved: {raw_file}')\n",
    "print(f'\\nShape: {df.shape[0]:,} rows √ó {df.shape[1]} columns')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-raw-profile-stage",
   "metadata": {},
   "source": [
    "## Stage 2.5 ‚Äî Data Profiling (Pre-Cleaning Baseline)\n",
    "\n",
    "Profiles the **raw master dataset** immediately after loading ‚Äî before any type casting,\n",
    "cleaning, deduplication, or validation takes place.\n",
    "\n",
    "This baseline snapshot serves two purposes:\n",
    "1. **Audit trail** ‚Äî documents the original data quality at ingestion time.\n",
    "2. **Before/after comparison** ‚Äî compare `profile_raw.csv` with `profile_cleaned.csv`\n",
    "   (Stage 7) to quantify exactly how many rows, nulls, and duplicates were removed.\n",
    "\n",
    "> ‚ö†Ô∏è `profile_dataset()` is defined in Stage 7. If running cells out of order, run Stage 7 first.\n",
    "> Numeric stats (price min/max/mean) will appear as NaN here ‚Äî type casting happens in Stage 3.\n",
    "\n",
    "> Output: `output/profiling/profile_raw.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-raw-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null counts are restricted to source columns only.\n",
    "# Derived cols (remaining_lease, price_anomaly, block_numeric, year_month)\n",
    "# are excluded to keep pre- and post-cleaning profiles comparable.\n",
    "SOURCE_COLS_FOR_PROFILING = [\n",
    "    'block', 'flat_model', 'flat_type', 'floor_area_sqm',\n",
    "    'lease_commence_date', 'month', 'resale_price',\n",
    "    'storey_range', 'street_name', 'town'\n",
    "]\n",
    "\n",
    "def profile_dataset(df, label='dataset'):\n",
    "    \"\"\"\n",
    "    Statistical profile for a DataFrame.\n",
    "    label : 'raw_master' or 'cleaned_final' ‚Äî stored in output for easy comparison.\n",
    "    Null counts cover SOURCE_COLS_FOR_PROFILING only (no derived columns).\n",
    "    \"\"\"\n",
    "    profile = {'profile_label': label}\n",
    "    profile['total_rows']    = len(df)\n",
    "    profile['total_columns'] = len(df.columns)\n",
    "\n",
    "    # Null counts ‚Äî source columns only, skip if column absent (raw vs cleaned differ)\n",
    "    for col in SOURCE_COLS_FOR_PROFILING:\n",
    "        if col in df.columns:\n",
    "            profile[f\"null_count_{col}\"] = int(df[col].isna().sum())\n",
    "\n",
    "    # Numeric distributions\n",
    "    for col in ['resale_price', 'floor_area_sqm']:\n",
    "        if col in df.columns:\n",
    "            profile[f\"{col}_min\"]    = df[col].min()\n",
    "            profile[f\"{col}_max\"]    = df[col].max()\n",
    "            profile[f\"{col}_mean\"]   = round(float(df[col].mean()), 2)\n",
    "            profile[f\"{col}_median\"] = df[col].median()\n",
    "\n",
    "    profile['duplicate_rows'] = int(df.duplicated().sum())\n",
    "    return profile\n",
    "\n",
    "profile_raw = timed_step('Profile Raw Dataset', profile_dataset, df, label='raw_master')\n",
    "path = os.path.join(PROFILE_OUT_DIR, 'profile_raw.csv')\n",
    "pd.DataFrame([profile_raw]).to_csv(path, index=False)\n",
    "\n",
    "print(f'üìä Post-cleaning profiling report saved: {path}')\n",
    "\n",
    "print('\\n‚îÄ‚îÄ Key Statistics (Cleaned) ‚îÄ‚îÄ')\n",
    "print(f\"  Rows          : {profile_raw['total_rows']:,}\")\n",
    "print(f\"  Columns       : {profile_raw['total_columns']}\")\n",
    "print(f\"  Duplicates    : {profile_raw['duplicate_rows']}\")\n",
    "print(f\"  Price min     : ${profile_raw['resale_price_min']:,.0f}\")\n",
    "print(f\"  Price max     : ${profile_raw['resale_price_max']:,.0f}\")\n",
    "print(f\"  Price mean    : ${profile_raw['resale_price_mean']:,.0f}\")\n",
    "print(f\"  Price median  : ${profile_raw['resale_price_median']:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-4961381224581248120",
   "metadata": {},
   "source": [
    "## Stage 3 ‚Äî Data Cleaning\n",
    "\n",
    "- Cast `month`, `resale_price`, `floor_area_sqm` to correct types\n",
    "- Drop rows with nulls in critical fields\n",
    "- Recompute `remaining_lease` from `lease_commence_date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-7224205749141854025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type casting\n",
    "df['month']          = pd.to_datetime(df['month'], format='%Y-%m', errors='coerce')\n",
    "df['resale_price']   = pd.to_numeric(df['resale_price'], errors='coerce')\n",
    "df['floor_area_sqm'] = pd.to_numeric(df['floor_area_sqm'], errors='coerce')\n",
    "\n",
    "before = len(df)\n",
    "df = df.dropna(subset=['month', 'resale_price', 'floor_area_sqm'])\n",
    "after = len(df)\n",
    "print(f'‚úì Type casting complete')\n",
    "print(f'  Rows dropped (null critical fields): {before - after:,}')\n",
    "print(f'  Rows remaining: {after:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-5373970644915005147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompute_remaining_lease(df):\n",
    "    today = pd.Timestamp.today()\n",
    "    def compute(row):\n",
    "        lease_start = row.get('lease_commence_date')\n",
    "        if pd.isna(lease_start):\n",
    "            return None\n",
    "        end = pd.Timestamp(year=int(lease_start), month=1, day=1) + pd.DateOffset(years=99)\n",
    "        if end < today:\n",
    "            return '0 years 0 months'\n",
    "        months_remaining = (end.year - today.year) * 12 + (end.month - today.month)\n",
    "        return f'{months_remaining//12} years {months_remaining%12} months'\n",
    "    df['remaining_lease'] = df.apply(compute, axis=1)\n",
    "    return df\n",
    "\n",
    "df = recompute_remaining_lease(df)\n",
    "print('‚úì remaining_lease recomputed')\n",
    "print(f'  Sample values:')\n",
    "print(df[['lease_commence_date','remaining_lease']].drop_duplicates().head(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-7590685506075398090",
   "metadata": {},
   "source": [
    "## Stage 4 ‚Äî Deduplication\n",
    "\n",
    "Identifies duplicate records where all fields except `resale_price` are identical.\n",
    "Keeps the row with the **higher** price and saves removed duplicates to audit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-3692723629831264241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_dataset(df):\n",
    "    \"\"\"\n",
    "    Remove duplicates sharing the same composite key (all columns except\n",
    "    resale_price). Keeps the higher-priced row; discards the lower.\n",
    "    Discarded rows are tagged with failure_reason for the failed dataset.\n",
    "    \"\"\"\n",
    "    key_cols   = [c for c in df.columns if c != 'resale_price']\n",
    "    df_sorted  = df.sort_values('resale_price', ascending=False)\n",
    "    df_cleaned = df_sorted.drop_duplicates(subset=key_cols, keep='first')\n",
    "    df_dupes   = df_sorted.loc[~df_sorted.index.isin(df_cleaned.index)].copy()\n",
    "    if not df_dupes.empty:\n",
    "        df_dupes['failure_reason'] = 'duplicate_key_lower_price'\n",
    "    return df_cleaned, df_dupes\n",
    "\n",
    "df_cleaned, df_duplicates = deduplicate_dataset(df)\n",
    "\n",
    "print(f'‚úì Deduplication complete')\n",
    "print(f'  Original rows : {len(df):,}')\n",
    "print(f'  Duplicates    : {len(df_duplicates):,}')\n",
    "print(f'  Clean rows    : {len(df_cleaned):,}')\n",
    "\n",
    "if not df_duplicates.empty:\n",
    "    path = os.path.join(AUDIT_OUT_DIR, 'duplicates.csv')\n",
    "    df_duplicates.to_csv(path, index=False)\n",
    "    print(f'  ‚ö†Ô∏è  Duplicates saved: {path}')\n",
    "else:\n",
    "    print('  ‚úì No duplicates found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-1337146713984008264",
   "metadata": {},
   "source": [
    "## Stage 5 ‚Äî Business Rule Validation\n",
    "\n",
    "Applies 6 domain-specific rules row by row. Any failing row is captured with a `comments` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-5713787363207121854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_validation(df):\n",
    "    \"\"\"\n",
    "    Vectorised business-rule validation. Applies each rule across the full\n",
    "    column at once rather than row-by-row ‚Äî far faster on large DataFrames.\n",
    "\n",
    "    Rules:\n",
    "    1. resale_price      > 0\n",
    "    2. floor_area_sqm    > 0 \n",
    "    3. town              in VALID_TOWNS\n",
    "    4. flat_type         in VALID_FLAT_TYPES\n",
    "    5. flat_model        in VALID_FLAT_MODELS\n",
    "    6. storey_range      matches DD TO DD format AND lower ‚â§ upper\n",
    "    7. month             within Mar 2012 ‚Äì Dec 2016\n",
    "    \"\"\"\n",
    "    issues = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Rule 1: resale_price > 0\n",
    "    issues['invalid_resale_price'] = df['resale_price'] <= 0\n",
    "\n",
    "    # Rule 2: floor_area_sqm > 0\n",
    "    issues['invalid_floor_area_sqm'] = (df['floor_area_sqm'] <= 0)\n",
    "\n",
    "    # Rules 3-5: categorical membership\n",
    "    issues['invalid_town']       = ~df['town'].isin(VALID_TOWNS)\n",
    "    issues['invalid_flat_type']  = ~df['flat_type'].isin(VALID_FLAT_TYPES)\n",
    "    issues['invalid_flat_model'] = ~df['flat_model'].isin(VALID_FLAT_MODELS)\n",
    "\n",
    "    # Rule 6: storey_range format AND logical order (lower storey ‚â§ upper storey)\n",
    "    fmt_ok = df['storey_range'].astype(str).str.match(VALID_STOREY_FORMAT, na=False)\n",
    "    def _logical_order(val):\n",
    "        m = re.match(VALID_STOREY_FORMAT, str(val))\n",
    "        if not m: return False\n",
    "        lo, hi = int(str(val)[:2]), int(str(val)[6:8])\n",
    "        return lo <= hi\n",
    "    logical_ok = df['storey_range'].apply(_logical_order)\n",
    "    issues['invalid_storey_range'] = ~(fmt_ok & logical_ok)\n",
    "\n",
    "    # Rule 7: month within expected range\n",
    "    month_period = df['month'].dt.to_period('M')\n",
    "    issues['month_out_of_range'] = (month_period < EXPECTED_START) | (month_period > EXPECTED_END)\n",
    "\n",
    "    # Build failed rows with consolidated comments\n",
    "    rule_cols = issues.columns.tolist()\n",
    "    any_fail  = issues.any(axis=1)\n",
    "    df_fail   = df.loc[any_fail].copy()\n",
    "    df_fail['comments']       = issues[any_fail].apply(\n",
    "        lambda row: '; '.join(col for col in rule_cols if row[col]), axis=1\n",
    "    )\n",
    "    df_fail['failure_reason'] = 'rule_violation'\n",
    "    return df_fail\n",
    "\n",
    "df_rules_fail = timed_step('Business Rule Validation', extra_validation, df_cleaned)\n",
    "\n",
    "print(f'‚úì Validation complete')\n",
    "print(f'  Rule violations : {len(df_rules_fail):,} rows')\n",
    "\n",
    "if not df_rules_fail.empty:\n",
    "    path = os.path.join(AUDIT_OUT_DIR, 'rule_violations.csv')\n",
    "    df_rules_fail.to_csv(path, index=False)\n",
    "    print(f'  ‚ö†Ô∏è  Violations saved: {path}')\n",
    "    print(df_rules_fail['comments'].value_counts().head(10))\n",
    "else:\n",
    "    print('  ‚úì No rule violations found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-6768001129970658899",
   "metadata": {},
   "source": [
    "## Stage 6 ‚Äî Anomaly Detection (3-Sigma)\n",
    "\n",
    "Detects statistically unusual resale prices using the **3-sigma (Z-score) method**.\n",
    "\n",
    "### Why 3-Sigma?\n",
    "Based on the Empirical Rule (68-95-99.7 Rule):\n",
    "\n",
    "| Sigma | Data Coverage | Frequency | Decision |\n",
    "|-------|--------------|-----------|----------|\n",
    "| 1œÉ | ~68% | 1 in 3 | Too sensitive |\n",
    "| 2œÉ | ~95% | 1 in 20 | Too many false positives |\n",
    "| **3œÉ** | **~99.7%** | **1 in 370** | **Selected threshold ‚úì** |\n",
    "\n",
    "### Why localised grouping (per Town + Flat Type)?\n",
    "Each flat is compared only against its own peer group (same town, same flat type) ‚Äî\n",
    "so a \\$900k Executive flat is never unfairly penalised for being more expensive than a 3-room flat next door.\n",
    "\n",
    "> **Assumption:** Prices within each Town + Flat Type group are approximately normally distributed.\n",
    "> If heavily skewed, consider Median Absolute Deviation (MAD) as a more robust alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-6969248881697908207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalous_prices(df):\n",
    "    \"\"\"\n",
    "    3-sigma anomaly detection per (town, flat_type) peer group.\n",
    "    Works on a copy ‚Äî never mutates the caller's DataFrame.\n",
    "    Groups with < 3 members are skipped (std is undefined/meaningless).\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    anomalies_list = []\n",
    "\n",
    "    for (town, flat_type), group in df_copy.groupby(['town', 'flat_type']):\n",
    "        if len(group) < 3:\n",
    "            continue\n",
    "        mean  = group['resale_price'].mean()\n",
    "        std   = group['resale_price'].std()\n",
    "        lower = mean - 3 * std\n",
    "        upper = mean + 3 * std\n",
    "        anomalies = group[\n",
    "            (group['resale_price'] < lower) | (group['resale_price'] > upper)\n",
    "        ].copy()\n",
    "        if not anomalies.empty:\n",
    "            anomalies['failure_reason'] = 'price_anomaly_3sigma'\n",
    "            anomalies_list.append(anomalies)\n",
    "\n",
    "    return pd.concat(anomalies_list) if anomalies_list else pd.DataFrame()\n",
    "\n",
    "df_anomalies = timed_step('Anomaly Detection', detect_anomalous_prices, df_cleaned)\n",
    "\n",
    "print(f'‚úì Anomaly detection complete')\n",
    "print(f'  Anomalous rows: {len(df_anomalies):,}')\n",
    "\n",
    "if not df_anomalies.empty:\n",
    "    path = os.path.join(AUDIT_OUT_DIR, 'anomalies.csv')\n",
    "    df_anomalies.to_csv(path, index=False)\n",
    "    print(f'  ‚ö†Ô∏è  Anomalies saved: {path}')\n",
    "    print('\\n  Top anomalous towns:')\n",
    "    print(df_anomalies['town'].value_counts().head(5))\n",
    "else:\n",
    "    print('  ‚úì No anomalies detected')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-7208711213571599069",
   "metadata": {},
   "source": [
    "## Stage 6b ‚Äî Combine Failed Records & Finalise Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-1683713986782011038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all failed records\n",
    "failed_records = pd.concat([df_duplicates, df_rules_fail, df_anomalies]).drop_duplicates()\n",
    "print(f'Total failed records (dupes + violations + anomalies): {len(failed_records):,}')\n",
    "\n",
    "if not failed_records.empty:\n",
    "    path = os.path.join(FAILED_OUT_DIR, 'hdb_resale_failed.csv')\n",
    "    failed_records.to_csv(path, index=False)\n",
    "    print(f'  ‚ö†Ô∏è  Failed records saved: {path}')\n",
    "\n",
    "# Remove failed from cleaned\n",
    "df_cleaned_final = df_cleaned.loc[~df_cleaned.index.isin(failed_records.index)]\n",
    "cleaned_file = os.path.join(CLEANED_OUT_DIR, 'hdb_resale_cleaned.csv')\n",
    "df_cleaned_final.to_csv(cleaned_file, index=False)\n",
    "\n",
    "print(f'\\nüíæ Cleaned dataset saved: {cleaned_file}')\n",
    "print(f'   Final rows: {len(df_cleaned_final):,}')\n",
    "df_cleaned_final.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-6348223152617364229",
   "metadata": {},
   "source": [
    "## Stage 7 ‚Äî Data Profiling (Post-Cleaning)\n",
    "\n",
    "Generates a statistical summary of the **final cleaned dataset** (after deduplication, validation, and anomaly removal).\n",
    "Compare `profile_cleaned.csv` against `profile_raw.csv` (produced in Stage 2.5) to quantify the cleaning impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-2489465299007983501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_dataset(df, label=\"dataset\"):\n",
    "    \"\"\"Statistical profile for a DataFrame. `label` distinguishes raw vs cleaned profiles.\"\"\"\n",
    "    profile = {'profile_label': label}\n",
    "    profile['total_rows']    = len(df)\n",
    "    profile['total_columns'] = len(df.columns)\n",
    "\n",
    "    # Null counts ‚Äî source columns only, skip if column absent (raw vs cleaned differ)\n",
    "    for col in SOURCE_COLS_FOR_PROFILING:\n",
    "        if col in df.columns:\n",
    "            profile[f\"null_count_{col}\"] = int(df[col].isna().sum())\n",
    "\n",
    "    # Numeric distributions\n",
    "    for col in ['resale_price', 'floor_area_sqm']:\n",
    "        if col in df.columns:\n",
    "            profile[f\"{col}_min\"]    = df[col].min()\n",
    "            profile[f\"{col}_max\"]    = df[col].max()\n",
    "            profile[f\"{col}_mean\"]   = round(float(df[col].mean()), 2)\n",
    "            profile[f\"{col}_median\"] = df[col].median()\n",
    "\n",
    "    profile['duplicate_rows'] = int(df.duplicated().sum())\n",
    "    return profile\n",
    "\n",
    "path = os.path.join(PROFILE_OUT_DIR, \"profile_cleaned.csv\")\n",
    "profile_clean = timed_step(\"Profile Cleaned Dataset\", profile_dataset, df_cleaned_final, label=\"cleaned_final\")\n",
    "pd.DataFrame([profile_clean]).to_csv(path, index=False)\n",
    "print(f\"üìä Post-cleaning profile saved: {path}\")\n",
    "print(f'üìä Profiling report saved: {path}')\n",
    "\n",
    "# Display key stats\n",
    "print('\\n‚îÄ‚îÄ Key Statistics ‚îÄ‚îÄ')\n",
    "print(f\"  Rows          : {profile_clean['total_rows']:,}\")\n",
    "print(f\"  Columns       : {profile_clean['total_columns']}\")\n",
    "print(f\"  Duplicates    : {profile_clean['duplicate_rows']}\")\n",
    "print(f\"  Price min     : ${profile_clean['resale_price_min']:,.0f}\")\n",
    "print(f\"  Price max     : ${profile_clean['resale_price_max']:,.0f}\")\n",
    "print(f\"  Price mean    : ${profile_clean['resale_price_mean']:,.0f}\")\n",
    "print(f\"  Price median  : ${profile_clean['resale_price_median']:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-28820051034802207",
   "metadata": {},
   "source": [
    "## Stage 8 ‚Äî Transformation: Resale Identifier\n",
    "\n",
    "Creates a synthetic `Resale Identifier` field encoding location, price context, and timing:\n",
    "\n",
    "```\n",
    "Format:  S + block_numeric(3) + avg_price_prefix(2) + month_num(2) + town_initial(1)\n",
    "Example: S042450303A\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-6995343723949094473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resale_identifier(df):\n",
    "    \"\"\"\n",
    "    Build the Resale Identifier per specification:\n",
    "      S + block_numeric(3) + avg_price_prefix(2) + month_num(2) + town_initial(1)\n",
    "\n",
    "    block_numeric : Remove ALL non-digit characters, zero-pad to 3, truncate to 3.\n",
    "    avg_price_prefix : First 2 digits of integer average resale_price\n",
    "                       grouped by (year-month, town, flat_type).\n",
    "    month_num     : Zero-padded 2-digit transaction month.\n",
    "    town_initial  : First character of the town name.\n",
    "\n",
    "    Intermediate columns (block_numeric, year_month) are dropped before returning.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    if 'block' not in df_copy.columns:\n",
    "        df_copy['block'] = '000'\n",
    "\n",
    "    # Remove ALL non-digit chars, zero-pad to 3, truncate to exactly 3 digits\n",
    "    df_copy['block_numeric'] = (\n",
    "        df_copy['block'].astype(str)\n",
    "        .str.replace(r'[^\\d]', '', regex=True)\n",
    "        .str.zfill(3)\n",
    "        .str[:3]\n",
    "    )\n",
    "\n",
    "    df_copy['year_month'] = df_copy['month'].dt.to_period('M')\n",
    "    avg_price = df_copy.groupby(['year_month', 'town', 'flat_type'])['resale_price'].transform('mean')\n",
    "\n",
    "    df_copy['Resale Identifier'] = (\n",
    "        'S' +\n",
    "        df_copy['block_numeric'] +\n",
    "        avg_price.astype(int).astype(str).str[:2] +\n",
    "        df_copy['month'].dt.month.astype(str).str.zfill(2) +\n",
    "        df_copy['town'].str[0]\n",
    "    )\n",
    "\n",
    "    # Drop intermediate helper columns so they don't pollute the output files\n",
    "    df_copy.drop(columns=['block_numeric', 'year_month'], inplace=True)\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def dedup_by_identifier(df):\n",
    "    \"\"\"\n",
    "    Second deduplication pass after Resale Identifier creation.\n",
    "    Two rows can share an identifier if block, town, month are identical and\n",
    "    their group average price rounds to the same 2-digit prefix.\n",
    "    Keep the higher price; discard the lower.\n",
    "    \"\"\"\n",
    "    df_sorted = df.sort_values('resale_price', ascending=False)\n",
    "    df_clean  = df_sorted.drop_duplicates(subset=['Resale Identifier'], keep='first')\n",
    "    df_dupes  = df_sorted.loc[~df_sorted.index.isin(df_clean.index)].copy()\n",
    "    if not df_dupes.empty:\n",
    "        df_dupes['failure_reason'] = 'duplicate_resale_identifier_lower_price'\n",
    "    return df_clean, df_dupes\n",
    "\n",
    "\n",
    "df_transformed = create_resale_identifier(df_cleaned_final)\n",
    "df_transformed, df_id_dupes = dedup_by_identifier(df_transformed)\n",
    "\n",
    "if not df_id_dupes.empty:\n",
    "    failed_path = os.path.join(FAILED_OUT_DIR, 'hdb_resale_failed.csv')\n",
    "    df_id_dupes.to_csv(failed_path, mode='a', header=not os.path.exists(failed_path), index=False)\n",
    "    print(f'  ‚ö†Ô∏è  Identifier duplicates: {len(df_id_dupes):,} rows appended to failed dataset')\n",
    "\n",
    "path = os.path.join(TRANSFORM_OUT_DIR, 'hdb_resale_transformed.csv')\n",
    "df_transformed.to_csv(path, index=False)\n",
    "print(f'üíæ Transformed dataset saved: {path}')\n",
    "print(f'   Final rows: {len(df_transformed):,}')\n",
    "print('\\nSample Resale Identifiers:')\n",
    "print(df_transformed[['town', 'flat_type', 'resale_price', 'Resale Identifier']].head(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-8035671662094914602",
   "metadata": {},
   "source": [
    "## Stage 9 ‚Äî Hashing\n",
    "\n",
    "Applies **SHA-256** hashing to the `Resale Identifier` field, producing `Resale Identifier Hashed`.\n",
    "\n",
    "- Anonymises the synthetic key while preserving uniqueness\n",
    "- Deterministic: same input always produces the same 64-character hex output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-1111392567481699851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_resale_identifier(df):\n",
    "    df['Resale Identifier Hashed'] = df['Resale Identifier'].apply(\n",
    "        lambda x: hashlib.sha256(str(x).encode()).hexdigest()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df_hashed = hash_resale_identifier(df_transformed)\n",
    "\n",
    "path = os.path.join(HASHED_OUT_DIR, 'hdb_resale_hashed.csv')\n",
    "df_hashed.to_csv(path, index=False)\n",
    "print(f'üíæ Hashed dataset saved: {path}')\n",
    "print('\\nSample hashes:')\n",
    "print(df_hashed[['Resale Identifier','Resale Identifier Hashed']].head(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-4027741408498021158",
   "metadata": {},
   "source": [
    "## ‚úÖ ETL Complete ‚Äî Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-6556797434884807871",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('‚úÖ HDB RESALE ETL PIPELINE COMPLETE')\n",
    "print('=' * 60)\n",
    "print(f'  Final rows     : {len(df_hashed):,}')\n",
    "print(f'  Final columns  : {len(df_hashed.columns)}')\n",
    "print(f'  Date range     : {df_hashed[\"month\"].min().date()} ‚Üí {df_hashed[\"month\"].max().date()}')\n",
    "print()\n",
    "print('Output files:')\n",
    "outputs = [\n",
    "    (RAW_OUT_DIR,       'hdb_resale_raw.csv'),\n",
    "    (CLEANED_OUT_DIR,   'hdb_resale_cleaned.csv'),\n",
    "    (TRANSFORM_OUT_DIR, 'hdb_resale_transformed.csv'),\n",
    "    (HASHED_OUT_DIR,    'hdb_resale_hashed.csv'),\n",
    "    (FAILED_OUT_DIR,    'hdb_resale_failed.csv'),\n",
    "    (AUDIT_OUT_DIR,     'duplicates.csv'),\n",
    "    (AUDIT_OUT_DIR,     'rule_violations.csv'),\n",
    "    (AUDIT_OUT_DIR,     'anomalies.csv'),\n",
    "    (PROFILE_OUT_DIR,   'profile_cleaned.csv'),\n",
    "]\n",
    "for folder, fname in outputs:\n",
    "    path = os.path.join(folder, fname)\n",
    "    status = '‚úì' if os.path.isfile(path) else '‚ö†Ô∏è '\n",
    "    print(f'  {status} {path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
